{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFfiHFGX9lbe",
        "outputId": "a3b81a1f-55e5-458e-cd04-58dda9164078"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 41.6MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.21MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 10.3MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 7.83MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "실험 진행: Activation: relu, BatchNorm: False, Init: kaiming\n",
            "테스트 정확도: 93.39%\n",
            "--------------------------------------------------\n",
            "실험 진행: Activation: relu, BatchNorm: False, Init: xavier\n",
            "테스트 정확도: 94.14%\n",
            "--------------------------------------------------\n",
            "실험 진행: Activation: relu, BatchNorm: False, Init: normal\n",
            "테스트 정확도: 89.52%\n",
            "--------------------------------------------------\n",
            "실험 진행: Activation: relu, BatchNorm: True, Init: kaiming\n",
            "테스트 정확도: 94.92%\n",
            "--------------------------------------------------\n",
            "실험 진행: Activation: relu, BatchNorm: True, Init: xavier\n",
            "테스트 정확도: 95.01%\n",
            "--------------------------------------------------\n",
            "실험 진행: Activation: relu, BatchNorm: True, Init: normal\n",
            "테스트 정확도: 95.14%\n",
            "--------------------------------------------------\n",
            "실험 진행: Activation: sigmoid, BatchNorm: False, Init: kaiming\n",
            "테스트 정확도: 91.58%\n",
            "--------------------------------------------------\n",
            "실험 진행: Activation: sigmoid, BatchNorm: False, Init: xavier\n",
            "테스트 정확도: 90.76%\n",
            "--------------------------------------------------\n",
            "실험 진행: Activation: sigmoid, BatchNorm: False, Init: normal\n",
            "테스트 정확도: 69.78%\n",
            "--------------------------------------------------\n",
            "실험 진행: Activation: sigmoid, BatchNorm: True, Init: kaiming\n",
            "테스트 정확도: 92.39%\n",
            "--------------------------------------------------\n",
            "실험 진행: Activation: sigmoid, BatchNorm: True, Init: xavier\n",
            "테스트 정확도: 92.51%\n",
            "--------------------------------------------------\n",
            "실험 진행: Activation: sigmoid, BatchNorm: True, Init: normal\n",
            "테스트 정확도: 93.23%\n",
            "--------------------------------------------------\n",
            "전체 실험 결과:\n",
            "Activation: relu, BatchNorm: False, Init: kaiming --> 93.39%\n",
            "Activation: relu, BatchNorm: False, Init: xavier --> 94.14%\n",
            "Activation: relu, BatchNorm: False, Init: normal --> 89.52%\n",
            "Activation: relu, BatchNorm: True, Init: kaiming --> 94.92%\n",
            "Activation: relu, BatchNorm: True, Init: xavier --> 95.01%\n",
            "Activation: relu, BatchNorm: True, Init: normal --> 95.14%\n",
            "Activation: sigmoid, BatchNorm: False, Init: kaiming --> 91.58%\n",
            "Activation: sigmoid, BatchNorm: False, Init: xavier --> 90.76%\n",
            "Activation: sigmoid, BatchNorm: False, Init: normal --> 69.78%\n",
            "Activation: sigmoid, BatchNorm: True, Init: kaiming --> 92.39%\n",
            "Activation: sigmoid, BatchNorm: True, Init: xavier --> 92.51%\n",
            "Activation: sigmoid, BatchNorm: True, Init: normal --> 93.23%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "# 디바이스 설정 (GPU 사용가능 시 GPU 사용)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 0. MNIST 데이터셋 로드 및 70% 학습 / 30% 테스트 분할\n",
        "transform = transforms.ToTensor()\n",
        "dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "train_size = int(0.7 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# 1. MLP 모델 정의: 2개의 은닉층, 각 은닉층 20개 노드\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, activation='relu', use_bn=False):\n",
        "        super(MLP, self).__init__()\n",
        "        self.use_bn = use_bn\n",
        "        # 활성화 함수 설정: 'relu' 혹은 'sigmoid'\n",
        "        if activation.lower() == 'relu':\n",
        "            self.act = nn.ReLU()\n",
        "        elif activation.lower() == 'sigmoid':\n",
        "            self.act = nn.Sigmoid()\n",
        "        else:\n",
        "            raise ValueError(\"지원하지 않는 활성화 함수입니다.\")\n",
        "\n",
        "        # 입력층 (28x28=784) -> 은닉층1 (20)\n",
        "        self.fc1 = nn.Linear(28*28, 20)\n",
        "        if use_bn:\n",
        "            self.bn1 = nn.BatchNorm1d(20)\n",
        "        # 은닉층1 (20) -> 은닉층2 (20)\n",
        "        self.fc2 = nn.Linear(20, 20)\n",
        "        if use_bn:\n",
        "            self.bn2 = nn.BatchNorm1d(20)\n",
        "        # 은닉층2 (20) -> 출력층 (10)\n",
        "        self.fc3 = nn.Linear(20, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)  # [batch_size, 784]로 펼치기\n",
        "        x = self.fc1(x)\n",
        "        if self.use_bn:\n",
        "            x = self.bn1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.fc2(x)\n",
        "        if self.use_bn:\n",
        "            x = self.bn2(x)\n",
        "        x = self.act(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# 4. 가중치 초기화 함수: 카이밍, 제이비어, 정규분포 초기화\n",
        "def initialize_weights(model, init_type='kaiming'):\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Linear):\n",
        "            if init_type.lower() == 'kaiming':\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "            elif init_type.lower() == 'xavier':\n",
        "                nn.init.xavier_normal_(m.weight)\n",
        "            elif init_type.lower() == 'normal':\n",
        "                nn.init.normal_(m.weight, mean=0.0, std=0.01)\n",
        "            else:\n",
        "                raise ValueError(\"지원하지 않는 초기화 방법입니다.\")\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "# 학습 및 평가 함수 (예제에서는 3 에포크 진행)\n",
        "def train_and_evaluate(model, train_loader, test_loader, epochs=3):\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # 학습 루프\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # 평가 루프\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "# 실험 설정: 활성화 함수, Batch Normalization 적용 여부, 가중치 초기화 방법\n",
        "activations = ['relu', 'sigmoid']\n",
        "batch_norm_options = [False, True]\n",
        "init_types = ['kaiming', 'xavier', 'normal']\n",
        "\n",
        "results = {}\n",
        "\n",
        "# 각 설정에 대해 모델 생성, 초기화, 학습, 평가 진행\n",
        "for act in activations:\n",
        "    for bn in batch_norm_options:\n",
        "        for init in init_types:\n",
        "            config = f\"Activation: {act}, BatchNorm: {bn}, Init: {init}\"\n",
        "            print(f\"실험 진행: {config}\")\n",
        "            model = MLP(activation=act, use_bn=bn)\n",
        "            initialize_weights(model, init_type=init)\n",
        "            acc = train_and_evaluate(model, train_loader, test_loader, epochs=3)\n",
        "            results[config] = acc\n",
        "            print(f\"테스트 정확도: {acc:.2f}%\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "print(\"전체 실험 결과:\")\n",
        "for config, acc in results.items():\n",
        "    print(f\"{config} --> {acc:.2f}%\")\n"
      ]
    }
  ]
}